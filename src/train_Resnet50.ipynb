{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a1a9b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from GetRecallMovie import *\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.models import resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b8a0791",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_top_view_movies():\n",
    "    #from full dataset\n",
    "    df = pd.read_csv('data_raw/ratings.csv')\n",
    "    top_view_movies = df.movieId.value_counts().index.values[:100]\n",
    "    return top_view_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fce3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/dataset_feature.csv\")\n",
    "top_view_movies = list(find_top_view_movies())\n",
    "def recall_movies(df):\n",
    "    df_cov = pd.read_csv('data/Movie2Movie.csv')\n",
    "    top_20_movies = df_covisitation_to_dict(df_cov)\n",
    "    movie_list = df.movieId.tolist()\n",
    "    unique_movie = list(dict.fromkeys(movie_list))\n",
    "    # gererate from movie2movie\n",
    "    movies_2 = list(itertools.chain(*[top_20_movies[id] for id in unique_movie if id in top_20_movies]))\n",
    "    recall_list1 = [[id, cnt] for id, cnt in Counter(movies_2).most_common(100) if id not in unique_movie]\n",
    "    recall_id = [i[0] for i in recall_list1]\n",
    "    # add top_view_movies to 100\n",
    "    index = 0\n",
    "    while len(recall_id)<100:\n",
    "        if top_view_movies[index] not in recall_id:\n",
    "            recall_id.append(top_view_movies[index])\n",
    "        index += 1\n",
    "    return recall_id\n",
    "\n",
    "def get_result(df):\n",
    "    df1 = df[:30]\n",
    "    recall_id = recall_movies(df1)\n",
    "    df2 = list(df[30:][\"movieId\"])\n",
    "    binary_list = [1 if item in df2 else 0 for item in recall_id]\n",
    "    res = pd.DataFrame({'movieId': recall_id, 'Lable': binary_list})\n",
    "    return res\n",
    "\n",
    "def get_dataset_id(df):\n",
    "    df_set = df.groupby('userId').apply(lambda x:get_result(x))\n",
    "    return df_set\n",
    "\n",
    "df_set = get_dataset_id(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "77a19012",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_set.to_csv(\"./data/dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06057b7d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/dataset_feature.csv\")\n",
    "train_ratio = 0.8\n",
    "unique_user = df[\"userId\"].unique()\n",
    "n = len(unique_user)\n",
    "np.random.shuffle(unique_user)\n",
    "train_user = unique_user[0:int(n*train_ratio)]\n",
    "test_user = unique_user[int(n*train_ratio):]\n",
    "train_df = df[df['userId'].isin(train_user)]\n",
    "test_df = df[df['userId'].isin(test_user)]\n",
    "train_df.to_csv(\"./data/train_set.csv\")\n",
    "test_df.to_csv(\"./data/test_set.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6180a25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "def split_train_test(X,y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "class MovieDataset(Dataset):\n",
    "    def __init__(self, csv_file, num_rows=100, num_column=4, transform=None):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.num_rows = num_rows\n",
    "        self.num_column = num_column\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) // self.num_rows\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start_idx = idx * self.num_rows\n",
    "        end_idx = start_idx + self.num_rows\n",
    "        data = self.data.iloc[start_idx:end_idx, self.num_column+1:]\n",
    "        label = self.data.iloc[start_idx:end_idx, self.num_column].values\n",
    "        data = torch.tensor(data.values, dtype=torch.float32)\n",
    "        label = torch.tensor(label,dtype=torch.float32)\n",
    "        return data, label\n",
    "\n",
    "batchsize = 1\n",
    "trainset = MovieDataset('./data/train_set.csv',transform=transforms.ToTensor())\n",
    "trainloader = torch.utils.data.DataLoader(trainset,batch_size=batchsize,shuffle=True)\n",
    "testset = MovieDataset('./data/test_set.csv',transform=transforms.ToTensor())\n",
    "testloader = torch.utils.data.DataLoader(testset,batch_size=batchsize,shuffle=True)\n",
    "input_channels = 1  \n",
    "custom_input_height = 100\n",
    "custom_input_width = 97\n",
    "num_classes = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4daacd2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|███████████████████████████████████████| 330/330 [00:25<00:00, 12.69batch/s, loss=0.513, acc=98.4, epoch=0]\n",
      "Test: 100%|██████████████████████████████████████████| 83/83 [00:01<00:00, 57.96batch/s, loss=0.499, acc=98.2, epoch=0]\n",
      "Train: 100%|███████████████████████████████████████| 330/330 [00:25<00:00, 12.89batch/s, loss=0.378, acc=99.1, epoch=1]\n",
      "Test: 100%|██████████████████████████████████████████| 83/83 [00:01<00:00, 52.60batch/s, loss=0.467, acc=98.9, epoch=1]\n",
      "Train: 100%|███████████████████████████████████████| 330/330 [00:25<00:00, 12.81batch/s, loss=0.294, acc=99.1, epoch=2]\n",
      "Test: 100%|██████████████████████████████████████████| 83/83 [00:01<00:00, 54.49batch/s, loss=0.401, acc=98.9, epoch=2]\n"
     ]
    }
   ],
   "source": [
    "model = resnet50(pretrained=True)\n",
    "model.conv1 = nn.Conv2d(input_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(model.fc.in_features, num_classes),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "top_n = 10\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(),lr=1e-3)\n",
    "num_epochs = 3\n",
    "train_loss_history = []\n",
    "train_acc_history = []\n",
    "val_loss_history = []\n",
    "val_acc_history = []\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss_train = 0.0\n",
    "    running_acc_train = 0.0\n",
    "#     if epoch == 5:\n",
    "#         for param in model.layer4.parameters():\n",
    "#             param.requires_grad = True\n",
    "#     elif epoch == 10:\n",
    "#         for param in model.layer3.parameters():\n",
    "#             param.requires_grad = True\n",
    "#     elif epoch == 15:\n",
    "#         optimizer = optim.Adam(model.parameters(),lr=1e-4)\n",
    "#     elif epoch == 20:\n",
    "#         optimizer = optim.Adam(model.parameters(),lr=1e-5)\n",
    "    with tqdm(total=len(trainloader), desc=f\"Train\", unit=\"batch\") as pbar:\n",
    "        for n_batch,data in enumerate(trainloader):\n",
    "            images,labels = data\n",
    "            images = torch.unsqueeze(images, 1)\n",
    "            images,labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs,labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss_train += loss.item()\n",
    "            outputs = outputs.squeeze().tolist()\n",
    "            labels = labels.squeeze().tolist()\n",
    "            \n",
    "            top_10_indices = np.argsort(outputs)[-top_n:]\n",
    "            count = 0\n",
    "            for index in top_10_indices:\n",
    "                if labels[index] == 1:\n",
    "                    count += 1\n",
    "            running_acc_train += count/top_n\n",
    "            pbar.set_postfix({'loss': loss.item(), 'acc': 100. * running_acc_train / (n_batch + 1), 'epoch': epoch})\n",
    "            pbar.update()\n",
    "        train_loss_history.append(running_loss_train/ len(trainloader))\n",
    "        train_acc_history.append(100. * running_acc_train / len(trainloader)) \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        running_loss_val = 0.0\n",
    "        running_acc_val = 0.0\n",
    "        with tqdm(total=len(testloader), desc=f\"Test\", unit=\"batch\") as pbar:\n",
    "            for n1_batch,data in enumerate(testloader):\n",
    "                images,labels = data\n",
    "                images = torch.unsqueeze(images, 1)\n",
    "                images,labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs,labels)\n",
    "                running_loss_val += loss.item()\n",
    "                outputs = outputs.squeeze().tolist()\n",
    "                labels = labels.squeeze().tolist()\n",
    "                top_10_indices = np.argsort(outputs)[-top_n:]\n",
    "                count = 0\n",
    "                for index in top_10_indices:\n",
    "                    if labels[index] == 1:\n",
    "                        count+=1\n",
    "                running_acc_val += count/top_n\n",
    "                pbar.set_postfix({'loss': loss.item(), 'acc': 100. * running_acc_val / (n1_batch + 1), 'epoch': epoch})\n",
    "                pbar.update()\n",
    "            val_loss_history.append(running_loss_val/len(testloader))\n",
    "            val_acc_history.append(100. * running_acc_val / len(testloader)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906f41de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-py3.9",
   "language": "python",
   "name": "ml-py3.9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
